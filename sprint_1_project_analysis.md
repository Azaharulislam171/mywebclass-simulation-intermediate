# Project Requirement Analysis


# Overview.

Hey there, MyWebClassers! It's your CEO Keith, and I'm back to talk to you about lean and agile practices. Don't worry, I promise to keep it funny!

So, imagine you're eating a giant pizza. You wouldn't try to shove the whole thing in your mouth at once, would you? (Unless you're trying to impress someone, but that's a different story.) No, you'd take small, manageable bites, savoring each delicious slice. That's exactly what we're going to do with our project tasks. We're going to break them down into small, manageable bites, savoring each delicious accomplishment along the way.

And just like toppings on a pizza, metrics add flavor and help us measure our progress. We'll be tracking metrics like deployment times, automated test times, user satisfaction scores, and even the number of high-fives we give each other for a job well done. That's right, folks, we're counting high-fives now. And just like how you measure the amount of cheese on a pizza, we'll measure our progress with these metrics to make sure we're on track to deliver a high-quality product on time and within budget.

Now, let's talk about the tasks themselves. We've identified a set of tasks that are critical for the success of our project. Think of these tasks like toppings on a pizza. Each topping adds flavor and texture to the pizza, just like each task adds value to our project. We'll be conducting legal research, implementing internationalization features, performing SEO research, and making the website accessible to all users. And just like how you might get creative with pizza toppings, we'll be getting creative with our tasks to deliver a website that our users will love.

But don't worry, folks, we're not just throwing a bunch of toppings on a pizza and hoping for the best. We'll be using lean and agile practices to streamline our workflow, increase collaboration and communication, and deliver high-quality work faster. It's like having a well-oiled pizza-making machine, except instead of pizza, we're making a kick-ass website.

So, let's get cooking, MyWebClassers! By adopting lean and agile practices, we can work more efficiently, collaborate more effectively, and deliver better results for our clients. And who doesn't want to deliver a website that's hot, fresh, and ready to be devoured by our users? Let's make this project a success and celebrate with a pizza party (or a high-five party, if that's more your style).



# Assignment Instructions

## Step 1 - Create a rough draft of Themes, Initiatives, and Epics for the project:

### Requirements

# MyWebClass.Org Sprint Tasks

As CEO of MyWebClass.Org, I believe that the tasks assigned for this sprint support both Agile and Lean business methodologies. Our company is focused on continuous improvement, and these tasks will help us achieve that goal.

## Front-End Testing

- Choose three different CSS frameworks and implement the homepage and content page using each one. Also, implement the same pages in plain HTML and CSS.
- Test the performance of Next.js by comparing it to three CSS frameworks and static pages.
- Use Playwright to automate browser testing of the pages that are currently in the project (homepage, content page, story).  You will need tests for next.js, Bootstrap, Tailwind and any other version of the site you make.
- Measure the following metrics: page load time, accessibility score, best practices score, first contentful paint (FCP), time to interactive (TTI), total blocking time (TBT), cumulative layout shift (CLS), page size, number of requests, bundle size, overall size, and other relevant statistics.
- For the final report, include industry statistics for each of these metrics, such as average page load time for websites in the same industry or the ideal score for accessibility and best practices.

## Backend Testing
- Test the Oracle server with Artilery and Playwright to evaluate next.js and each of the css frameworks 

## DevOps Tools Testing and Evaluation

- Test and evaluate different node package managers to determine which one is the fastest. NPM Yarn and otherss
- Test and evaluate build tools like Webpack, Veet, and EsBuild to determine their efficiency.
- Implement and check the DevOps pipeline using GitHub Actions for quality control 
- Inlcuee checks for coding standards relevant to the project  JavaScript, CSS, and HTML.
- Generate a LightHouse Report with videos and screen shots of failed tests when GitHub actions run and automatically upload it to view in GitHub Actions. Use Playwright to hit each page to run Lighthouse.
- Automatically test W3C validation, handicap accessibility, and JavaScript style.
- Run CodeQL using a GitHub action and upload the report to GitHub Actions.
- Run ESLint and upload the report to GitHub Actions.
- Measure the following metrics: build time, bundle size, code quality score, and other relevant statistics.
- For the final report, include industry statistics for each of these metrics, such as average build time for similar projects or the ideal bundle size for websites in the same industry.
- Create easy-to-follow instructions for local installation and how the developer should use the tools above.
- Measure the following metrics: deployment time,  developer environment setup time, test running time, and other relevant statistics.
- Create a document that explains your tools and how you use them as well as the results of your test showing the relevant metrics you collected.


## Management

- Identify metrics for each category: Bundle (Webpack),  Test (Playwright),and Application performance.
- Measure the following metrics: user capacity, concurrent user capacity, storage requirements, bandwidth requirements, and other relevant statistics.
- For the final report, include industry statistics for each of these metrics, such as average user capacity for websites in the same industry or the ideal storage and bandwidth requirements for a given number of users.
- Keep a log for each metric in a GOogle sheet and link to it on your Readme.  USe separate sheets for each group of metrics.  Make it easy to maintain and update. 

By completing these tasks, we will be better equipped to deliver high-quality content to our users while improving our development process and optimizing our infrastructure. Good luck and have fun, everyone!



## Step 2 - Break Down Epics into Smaller User Stories:

- Divide the epics into smaller user stories to make it easier to manage the project and track progress.
    - Write down each user story separately.
    - Each Story should be written from the customer or business point of view and consist of one measurable task i.e.
      it can be tested manually or automated

## Step 3 - Prioritize User Stories:

- Determine the importance and urgency of each user story.
- Prioritize the user stories based on their significance and start with the most critical ones.

## Step 4 - Estimate Effort Required:

- Estimate the amount of time and resources needed to complete each user story.
- Note down the estimation for each user story.

## Step 5 - Assign Story Points:

- Assign story points to each user story based on its estimated effort required.
- Use the Fibonacci sequence (1, 2, 3, 5, 8, 13, etc.) to assign story points to each user story.

## Step 6 - Define Acceptance Criteria:

- Create a list of acceptance criteria for each user story.
- Clearly define what must be accomplished for each user story to be considered complete.
- Define positive and negative situations such as user entering in wrong data o making a mistake somehow

## Step 7 - Assign the user story to a team member and set a deadline for the task:

- Keep track of the progress of the project regularly.
- Ensure that user stories are being completed on time and according to the defined acceptance criteria.
- If any issues arise, make necessary adjustments to the project plan.
- Set deadlines early, so you have time to account for unexpected situations. Consider using Google Calendar or some
  other way to keep track of time. GitHub Project isn't very good at visualizing this
- Each Team Member is responsible for adding their user stories as issues to the project board in GitHub.
    - Take a screenshot after you load all your tasks into the project board and include it in the project analysis
      report.
## Step 8 - Hold a Sprint Retrospective Meeting or Discuss on the Discussion Board In Canvas
- Discuss how you can improve the requirement analysis and creation of the agile documents for the next sprint

# Submission Instructions
- Create a PowerPoint with your team during a meeting to capture your ideas for the agile documentation.
- All of your project documentation should be added to your repository's wiki see [here](https://docs.github.com/en/communities/documenting-your-project-with-wikis/about-wikis) for instructions on github wiki.
- Each team member must create a document and include a picture of their tasks after they initially add them to the task board
- Add a link to your project wiki to the document 
- Write a summary of your sprint retrospective for how your can improve this project requirement analysis and documentation writing in the future. 
- Submit the documents to canvas

# Grading Rubric
# Grading Rubric

| Rubric Criteria                                   | Unsatisfactory (0) | Satisfactory (1) | Exceeds Expectations (2) |
|---------------------------------------------------|-------------------|------------------|--------------------------|
| Breakdown of epics into smaller user stories       | 0                 | 1                | 2                        |
| Prioritization of user stories based on urgency    | 0                 | 1                | 2                        |
| and importance                                    |                   |                  |                          |
| Estimation of time and resources needed to         | 0                 | 1                | 2                        |
| complete user stories                             |                   |                  |                          |
| Assignment of story points using the Fibonacci    | 0                 | 1                | 2                        |
| sequence                                           |                   |                  |                          |
| Definition of acceptance criteria for each user   | 0                 | 1                | 2                        |
| story                                             |                   |                  |                          |
| Timely assignment of user stories and tracking     | 0                 | 1                | 2                        |
| progress                                          |                   |                  |                          |
| Participation in the Sprint Retrospective Meeting | 0                 | 1                | 2                        |
| or Discussion Board                               |                   |                  |                          |
| Total Points                                      | 0-4               | 5-9              | 10-20                    |



# Project Analysis Scenario
## Project Introduction: Spoken by the CEO

Hey there, web wizards and tech enthusiasts! As the CEO of MyWebClass.org, I am excited to embark on a journey to create
a website that will knock your socks off! But how do we ensure that we create a website that truly meets the needs of
our users while delivering a top-notch experience? Let me tell you, it all starts with a solid foundation.

By focusing on the design and basic requirements of the website and limiting the use of technology early on, we can
build a foundation that's easier to build upon later. And you know what? We'll be moving at lightning speed through the
early stages of development thanks to static pages that require less technical expertise!

But wait, there's more! We're also adopting a lean methodology that prioritizes the most valuable features and functions
based on user needs. No unnecessary complexity or features that don't add value here! And with the help of our trusty
user feedback, we'll be creating a more user-friendly and effective website.

Now, let's talk project management! We're going agile, baby! And that means we've got some seriously detailed
documentation at each level: themes, initiatives, epics, and user stories. But don't worry, we'll make it fun with some
acceptance criteria to measure if we're meeting our goals and delivering a product that meets your needs.

And we can't forget about story points! These little gems are a way of measuring the complexity of a task or user story,
helping us estimate the effort required to complete the work. It's like a game of difficulty ratings, and we're rating
our legal research user stories at a 5-point task for some moderate complexity and our terms and conditions user stories
at a 3-point task for less complexity.

So there you have it, folks! We're building a solid foundation, prioritizing your needs, and having some fun with our
agile project management. Let's create a website that blows your mind!



# Final Report: Front End Testing


# Front End Testing

## Introduction

The purpose of this report is to summarize the results of our front-end testing activities. We conducted tests on three different CSS frameworks, as well as plain HTML and CSS. We also tested the performance of Next.js, used Playwright for browser automation testing, Artillery for load testing, and Lighthouse for generating performance, accessibility, and best practices reports.

## Results

### CSS Framework Testing

We chose three different CSS frameworks and implemented the homepage and content page using each one. We also implemented the same pages in plain HTML and CSS. The following metrics were measured:

- Page load time
- Accessibility score
- Best practices score
- First contentful paint (FCP)
- Time to interactive (TTI)
- Total blocking time (TBT)
- Cumulative layout shift (CLS)
- Page size
- Number of requests

We compared the metrics for the different CSS frameworks and included industry statistics for each of these metrics in the table.

### Next.js Testing

We tested the performance of Next.js by comparing it to three CSS frameworks and static pages. The following metrics were measured:

- Page load time
- Accessibility score
- Best practices score
- First contentful paint (FCP)
- Time to interactive (TTI)
- Total blocking time (TBT)
- Cumulative layout shift (CLS)
- Page size
- Number of requests

We included industry statistics for each of these metrics in the table.

### Playwright Testing

We used Playwright for browser automation testing of the pages that are currently in the project (homepage, content page, story). The following metrics were measured:

- Page load time
- Page size
- Number of requests

We included the metrics for each page in the table.

### Artillery Testing

We used Artillery to load test the Oracle server with Playwright until it fails. We tested on AMD Oracle Server 1 or 2 cores, ARM 1 or 2 or 4 cores. The following metrics were measured:

- Configuration
- Maximum concurrent users
- Average page load time
- Bundle size
- Overall size

We included the metrics in the table.

### Lighthouse Testing

We used Lighthouse to generate performance, accessibility, and best practices reports. The reports were saved in a file format that is easy to view online. The following metrics were measured:

- Page load time
- Accessibility score
- Best practices score

We included the metrics in the table.

## Final Report

For the final report, we included industry statistics for each of the metrics measured in the different tests. The report includes the following parts:

- Introduction
- Results for CSS Framework Testing
- Results for Next.js Testing
- Results for Playwright Testing
- Results for Artillery Testing
- Results for Lighthouse Testing
- Conclusion

Each results section includes a table with the measured metrics and the industry statistics for each metric. The conclusion summarizes the overall performance of the different technologies tested and makes recommendations for improvement based on the results.



## Introduction

The purpose of this report is to summarize the results of our front end testing activities. We conducted tests on three different CSS frameworks, as well as plain HTML and CSS, and compared their performance to Next.js and static pages. We also used Playwright for browser automation testing, Artillery for load testing, and Lighthouse for generating performance, accessibility, and best practices reports.

## Raw Data Example - Modify to add or remove statistics as needed this is only an example.

### CSS Framework Testing

| Framework | Average Load Time (seconds) | Accessibility Score (out of 100) | Best Practices Score (out of 100) | Page Size (MB) | Number of Requests |
| --- | --- | --- | --- | --- | --- |
| Framework A | 2.5 | 95 | 97 | 1.2 | 75 |
| Framework B | 3.2 | 93 | 96 | 1.5 | 85 |
| Framework C | 4.2 | 80 | 91 | 2.1 | 105 |
| Plain HTML and CSS | 3.7 | 87 | 95 | 1.8 | 95 |

We found that the fastest CSS framework in terms of page load time was Framework A, which had an average load time of 2.5 seconds. The slowest CSS framework was Framework C, with an average load time of 4.2 seconds. We also found that Framework A had the highest accessibility score, with a score of 95 out of 100, while Framework C had the lowest accessibility score, with a score of 80 out of 100. In terms of best practices, all three CSS frameworks had scores above 90 out of 100, with Framework B scoring the highest at 97.

### Next.js Testing

| Type of Page | Average Load Time (seconds) | Accessibility Score (out of 100) | Best Practices Score (out of 100) | Page Size (MB) | Number of Requests |
| --- | --- | --- | --- | --- | --- |
| Next.js | 1.8 | 98 | 99 | 0.8 | 55 |
| Static Page | 3.5 | 92 | 94 | 1.6 | 90 |

Next.js performed significantly better than the CSS frameworks and static pages in terms of page load time, with an average load time of 1.8 seconds. Next.js also had the highest accessibility and best practices scores, with scores of 98 and 99 out of 100, respectively.

### Playwright Testing

| Page | Average Load Time (seconds) | Page Size (MB) | Number of Requests |
| --- | --- | --- | --- |
| Homepage | 2.5 | 1.2 | 75 |
| Content Page | 3.5 | 1.6 | 90 |
| Story | 4.2 | 2.1 | 105 |

Using Playwright for browser automation testing, we found that the pages in the project loaded within an acceptable range of time, with an average load time of 3 seconds. However, we also found that the pages had a high number of requests, with an average of 80 requests per page.

### Artillery Testing

| Configuration | Maximum Concurrent Users | Average Page Load Time (seconds) | Bundle Size (MB) | Overall Size (MB) |
| --- | --- | --- | --- | --- |
| AMD Oracle Server 1 core | 50 | 3.5 | 1.8 | 

